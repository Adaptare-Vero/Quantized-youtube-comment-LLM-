{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-K-_mzHyGNA"
      },
      "outputs": [],
      "source": [
        "!pip install auto-gptq\n",
        "!pip install optimum\n",
        "!pip install bitsandbytes\n",
        "\n",
        "# resolving \"No inf checks were recorded for this optimizer.\" issue\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==2.1\n",
        "\n",
        "!pip install torch torchvision torchaudio --upgrade\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
        "                                             trust_remote_code=False, # prevents running custom model files on your machine\n",
        "                                             revision=\"main\") # which version of model to use in repo\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model.eval() # model in evaluation mode (dropout modules are deactivated)\n",
        "\n",
        "# craft prompt\n",
        "comment = \"Great content, please can you elaborate on what datascience is, thank you!\"\n",
        "prompt=f'''[INST] {comment} [/INST]'''\n",
        "\n",
        "# tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()  # Dropout and batchnorm layers are deactivated\n",
        "\n",
        "# Craft a prompt\n",
        "comment = \"What are the latest trends in datascience and artificial intelligence?\"\n",
        "prompt = f'''[INST]\"{comment}\"[/INST]'''\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=100)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "model.eval()  # Ensures consistent predictions during inference\n",
        "\n",
        "# Create a prompt\n",
        "question = \"How can I build a recommendation system for e-commerce?\"\n",
        "prompt = f'''[INST] Provide a step-by-step guide to: \"{question}\" [/INST]'''\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=150)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "model.eval()  # Ensures consistent predictions during inference\n",
        "\n",
        "# Create a prompt\n",
        "question = \"How do algorithms optimize the training of large language models?\"\n",
        "prompt = f'''[INST] \"{question}\" [/INST]'''\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=150)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)\n",
        "\n",
        "# tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "# tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "model.train() # model in training mode (dropout modules are activated)\n",
        "\n",
        "# enable gradient check pointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# enable quantized training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA trainable version of model\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# trainable parameter count\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# load dataset\n",
        "data = load_dataset(\"shawhin/shawgpt-youtube-comments\")\n",
        "\n",
        "# create tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # extract text\n",
        "    text = examples[\"example\"]\n",
        "\n",
        "    #tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# tokenize training and validation datasets\n",
        "tokenized_data = data.map(tokenize_function, batched=True)\n",
        "\n",
        "# setting pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# data collator\n",
        "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "# define training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir= \"shawgpt-ft\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "\n",
        ")\n",
        "\n",
        "# configure trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "# train model\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "\n",
        "# renable warnings\n",
        "model.config.use_cache = True\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# # option 2: key login\n",
        "# from huggingface_hub import login\n",
        "# write_key = 'hf_' # paste token here\n",
        "# login(write_key)\n",
        "\n",
        "hf_name = 'shadow0017' # your hf username or org name\n",
        "model_id = hf_name + \"/\" + \"shawgpt-ft\"\n",
        "\n",
        "intstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\n",
        "ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        "\n",
        "comment = \"Great content, please can you elaborate on what datascience is, thank you!\"\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "comment = \"Great content, please can you elaborate on what datascience is, thank you!\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "model.eval()\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "\n",
        "# Define comment and use the prompt template\n",
        "comment = \"What are the latest trends in datascience and artificial intelligence?\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()  # Deactivates dropout and other non-deterministic layers\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "\n",
        "# Define comment and use the prompt template\n",
        "comment = \"How can I build a recommendation system for e-commerce?\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()  # Deactivates dropout and other non-deterministic layers\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "# Define comment and use the prompt template\n",
        "comment = \"How do algorithms optimize the training of large language models?\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()  # Deactivates dropout and other non-deterministic layers\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n",
        "\n",
        "\n",
        "# Define comment and apply the prompt template\n",
        "comment = \"How do algorithms optimize the training of large language models?\"\n",
        "prompt = prompt_template(comment)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])\n"
      ]
    }
  ]
}